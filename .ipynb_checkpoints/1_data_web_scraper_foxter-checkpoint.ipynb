{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "\n",
    "### Summary\n",
    "\n",
    "The purpose of this notebook is collect data from Foxter Real State company site on the Internet using a Web Scraper. This is the first step into the project to estimate prices of apartaments in Porto Alegre (Brazil).\n",
    "\n",
    "In this notebook, the Web Scraper will:\n",
    "1. download the sitemap.xml;\n",
    "2. visit each page in sitempa.xml file;\n",
    "3. collect data of each page, but some pages will be ignored (some conditionals if are specified);\n",
    "4. in each page visited, the scaper will collect new address of new pages;\n",
    "5. at the end, all data collected will be saved in a pandas dataframe.\n",
    "\n",
    "\n",
    "### Main Strategy\n",
    "\n",
    "The Web Scraper was constructed, in its main core, with BeautifulSoup, Requests and Pandas. The [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) turned easy to collect the data from every page. The [Requests](https://2.python-requests.org/en/master/) library made the connection possible to get the data. And [Pandas](https://pandas.pydata.org) was used to save the data collected into a sigle dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import date\n",
    "try:\n",
    "    from urlparse import urljoin  # Python2\n",
    "except ImportError:\n",
    "    from urllib.parse import urljoin  # Python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading sitemap.xml...\n",
      "sitemap.xml ok!\n"
     ]
    }
   ],
   "source": [
    "print('downloading sitemap.xml...')\n",
    "url=\"https://www.foxterciaimobiliaria.com.br/sitemap.xml\"\n",
    "response = requests.get(url)\n",
    "with open('sitemap.xml', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print('sitemap.xml ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(tocrawl,newlinks):\n",
    "    for e in newlinks:\n",
    "        if e not in tocrawl:\n",
    "            tocrawl.append(e)\n",
    "    return   \n",
    "\n",
    "                  \n",
    "# Find all links into content Soup and save \"alllinks\"\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    for link in page_content(page).find_all('a'):\n",
    "        links.append(str(link.get('href')))\n",
    "    return links\n",
    "                  \n",
    "                  \n",
    "def check_links(alllinks):\n",
    "    links_checked = []\n",
    "\n",
    "    for link_to_check in alllinks:\n",
    "\n",
    "        if link_to_check[:38] == url_seed:\n",
    "            links_checked.append(link_to_check)\n",
    "        \n",
    "        if link_to_check[:8] == url_segment:\n",
    "            link_to_check_join = urljoin(url_seed, link_to_check)\n",
    "            links_checked.append(link_to_check_join)\n",
    "            \n",
    "    return links_checked    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect content from one page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all data from a page using BeautifullSoup. The function will inform if the connecting maybe refused by the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_content(page):\n",
    "    \n",
    "    # if server refuse\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        wait = 0\n",
    "        try:\n",
    "            response = requests.get(page)\n",
    "        except:\n",
    "            wait = random.randint(10,2000) # in case if the server does not respond.\n",
    "            print(\"Connection refused by the server..\")\n",
    "            print(\"Let me sleep for \" + str(wait) + \" seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            time.sleep(wait)\n",
    "            print(\"Was a nice sleep, now let me continue...\")\n",
    "            continue\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # ignore page without id\n",
    "    if soup.find('span', {'itemprop':\"identifier\"}) is None:\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    #collect id\n",
    "    id_imovel = soup.find('span', {'itemprop':\"identifier\"})\n",
    "    id_clean = id_imovel.get_text()[7:]\n",
    "    id_list.append(id_clean)\n",
    "        \n",
    "        \n",
    "    #collect price\n",
    "    try:        \n",
    "        price = soup.find('span', {'itemprop': \"price\"})\n",
    "        price_clean0 = price.get_text()[3:]\n",
    "        price_clean = price_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        price_clean = 'NA'\n",
    "    price_list.append(price_clean)\n",
    "    \n",
    "        \n",
    "    #collect private area\n",
    "    try:\n",
    "        area = soup.find(string='Área privativa').next_element.next_element\n",
    "        area_clean = area.get_text()[:-2]\n",
    "    except:\n",
    "        area_clean = 'NA'        \n",
    "    area_list.append(area_clean)\n",
    "        \n",
    "        \n",
    "    #collect neighborhood\n",
    "    try:\n",
    "        district = soup.find(string='Bairro').next_element.next_element\n",
    "        district_clean = district.get_text()\n",
    "    except:\n",
    "        district_clean = 'NA'\n",
    "    district_list.append(district_clean)\n",
    "        \n",
    "        \n",
    "    #collect city\n",
    "    try:\n",
    "        city = soup.find(string='Cidade').next_element.next_element\n",
    "        city_clean = city.get_text()\n",
    "    except:\n",
    "        city_clean = 'NA'\n",
    "    city_list.append(city_clean)\n",
    "\n",
    "        \n",
    "    #collect type\n",
    "    try:\n",
    "        type1 = soup.find('span', {'itemprop': \"category\"})\n",
    "        type1_clean = type1.get_text()\n",
    "    except:\n",
    "        type1_clean = 'NA'\n",
    "    type1_list.append(type1_clean)\n",
    "\n",
    "\n",
    "    #collect segment\n",
    "    try:\n",
    "        segment = soup.find(string='Segmento').next_element.next_element\n",
    "        segment_clean = segment.get_text()\n",
    "    except:\n",
    "        segment_clean = 'NA'\n",
    "    segment_list.append(segment_clean)\n",
    "\n",
    "        \n",
    "    #collect condominium price/value\n",
    "    try:\n",
    "        condominium = soup.find(string='Condomínio').next_element.next_element\n",
    "        condominium_clean0 = condominium.get_text()[3:]\n",
    "        condominium_clean = condominium_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        condominium_clean = 'NA'\n",
    "    condominium_list.append(condominium_clean)\n",
    "\n",
    "        \n",
    "    #collect tax - IPUT\n",
    "    try:\n",
    "        iptu = soup.find(string='IPTU Anual').next_element.next_element\n",
    "        iptu_clean0 = iptu.get_text()[3:]\n",
    "        iptu_clean = iptu_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        iptu_clean = 'NA'\n",
    "    iptu_list.append(iptu_clean)\n",
    "    \n",
    "        \n",
    "    #collect rooms\n",
    "    try:\n",
    "        rooms = soup.find(string='Dormitórios').next_element.next_element\n",
    "        rooms_clean = rooms.get_text()\n",
    "    except:\n",
    "        rooms_clean = 'NA'\n",
    "    rooms_list.append(rooms_clean)\n",
    "\n",
    "        \n",
    "    #collect park space quantity\n",
    "    try:\n",
    "        box = soup.find('span', {'class': \"value vagas\"})\n",
    "        box_clean = box.get_text()\n",
    "    except:\n",
    "        box_clean = 'NA'\n",
    "    box_list.append(box_clean)\n",
    "                  \n",
    "        \n",
    "    #collect url\n",
    "    url_list.append(page)\n",
    "    \n",
    "    \n",
    "    #save data\n",
    "    date_list.append(date_today)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing visit and capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A group of \"if condition\" is necessary to specify what to not collect. At the end, all lists are joined into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(url_seed):\n",
    "    data_base = {}\n",
    "    tocrawl = [url_seed]\n",
    "    crawled = []\n",
    "    \n",
    "    # reading the xml file\n",
    "    with open('sitemap.xml') as file_object:\n",
    "        print('reading sitemap.xml ...')\n",
    "        for line in file_object:\n",
    "            soup = BeautifulSoup(line, \"html.parser\")\n",
    "        \n",
    "            for link in soup.find_all('loc'):\n",
    "                tocrawl.append(str(link.get_text())) \n",
    "\n",
    "    print(\"xml reading finished ... starting search ...\\n\")\n",
    "    \n",
    "    while tocrawl:\n",
    "\n",
    "        page = tocrawl.pop()\n",
    "\n",
    "        sys.stdout.write(\"\\rpages analised: {}/{}.\".format(len(crawled), (len(crawled) + len(tocrawl))))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "        if page not in crawled:\n",
    "            \n",
    "            # ingnore page withou \"http\"\n",
    "            if page[:5] != \"http:\":\n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignore pages from \"construtora\"\n",
    "            if \"/construtora\" in page:\n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignore pages from \"empreendimento\"\n",
    "            if '/empreendimento' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignore pages from \"bairro\"\n",
    "            if '/bairro' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignore pages with url error\n",
    "            if '.com.brhttps:' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "                       \n",
    "            # delimiting the search for three neighborhoods of interest:\n",
    "            # auxiliadora, bela vista, mont serrat\n",
    "            if \"-porto-alegre-auxiliadora-apartamento-\" not in page:\n",
    "                if \"-porto-alegre-bela-vista-apartamento-\" not in page:\n",
    "                    if \"-porto-alegre-mont-serrat-apartamento-\" not in page:\n",
    "                        crawled.append(page)\n",
    "                        continue               \n",
    "\n",
    "            union(tocrawl, check_links(get_all_links(page)))\n",
    "            crawled.append(page)            \n",
    "            \n",
    "            # join list into a dataframe\n",
    "            data_base = pd.DataFrame({'id': id_list,\n",
    "                                      'price': price_list,\n",
    "                                      'area': area_list,\n",
    "                                      'district': district_list,\n",
    "                                      'city': city_list,\n",
    "                                      'type': type1_list,\n",
    "                                      'segment': segment_list,\n",
    "                                      'condominium': condominium_list,\n",
    "                                      'iptu': iptu_list,\n",
    "                                      'rooms': rooms_list,\n",
    "                                      'box': box_list,\n",
    "                                      'url': url_list,\n",
    "                                      'date': date_list\n",
    "                                     })\n",
    "            \n",
    "            data_base.to_csv((date_today + '-foxter.csv'), sep='\\t') #salvando csv\n",
    "            \n",
    "    print(\"\\nall adresses finished!!\")\n",
    "    print(\"\\ntotal apartaments saved: \" + str(data_base.shape[0]))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraper Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the web scraper start with a url seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading sitemap.xml ...\n",
      "xml reading finished ... starting search ...\n",
      "\n",
      "pages analised: 27544/27544.\n",
      "all adresses finished!!\n",
      "\n",
      "total apartaments saved: 595\n"
     ]
    }
   ],
   "source": [
    "# data used to identify the collected data file.\n",
    "date_today = str(date.today())\n",
    "\n",
    "# data to capture: lists initialization\n",
    "id_list = []\n",
    "price_list = []\n",
    "area_list = []\n",
    "district_list = []\n",
    "city_list = []\n",
    "type1_list = []\n",
    "segment_list = []\n",
    "condominium_list = []\n",
    "iptu_list = []\n",
    "rooms_list = []\n",
    "box_list = []\n",
    "url_list = []\n",
    "date_list = []\n",
    "\n",
    "\n",
    "\n",
    "url_seed = (\"http://www.foxterciaimobiliaria.com.br\") # Initial page - seed\n",
    "\n",
    "url_segment = '/imovel/' # identify what kind of segment into the site\n",
    "\n",
    "\n",
    "\n",
    "crawl_web(url_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
